{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/87d9a1e930b5b813/reinforcement_learning/reinforce.py\n",
    "- https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf [Notebook](https://nbviewer.jupyter.org/urls/gist.githubusercontent.com/ts1829/ebbe2cf946bf36951b724818c52e36b9/raw/4da449bffe9835e201f2fb34f381fbb53568d1ca/Policy%20Gradient%20with%20Cartpole%20and%20PyTorch%20%28Medium%20Version%29.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import itertools\n",
    "from collections import namedtuple, defaultdict\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from tuplestate import *\n",
    "from gamestate import *\n",
    "from benchmarking import *\n",
    "from vectorize import *\n",
    "random.seed(0)\n",
    "# print(to_pretty_string(klonstate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All seeds\n",
      "Solved-Min      7,876\n",
      "Solved            320\n",
      "Impossible      1,282\n",
      "Unknown           522\n",
      "---------------------\n",
      "Total          10,000\n"
     ]
    }
   ],
   "source": [
    "all_solutions = os.listdir('./bench/shootme/')\n",
    "\n",
    "def solve_state(ret):\n",
    "    lines = ret.splitlines()\n",
    "    result = lines[15]\n",
    "    if result.startswith('Minimal solution'):\n",
    "        return \"Solved-Min\"\n",
    "    elif result.startswith(\"Solved\"):\n",
    "        return \"Solved\"\n",
    "    elif result.startswith('Impossible'):\n",
    "        return \"Impossible\"\n",
    "    elif result.startswith('Unknown'):\n",
    "        return \"Unknown\"\n",
    "\n",
    "def clf_seeds(seedlist):\n",
    "    results = defaultdict(set)\n",
    "    for seed in seedlist:\n",
    "        with open(f\"./bench/shootme/{seed}\") as f:\n",
    "            ret = f.read()\n",
    "            result = solve_state(ret)\n",
    "            results[result].add(seed)\n",
    "    return results\n",
    "\n",
    "def clf_summary(seedlist):\n",
    "    results = clf_seeds(seedlist)\n",
    "    states = ['Solved-Min', 'Solved', 'Impossible', 'Unknown']\n",
    "    for clfstate in states:\n",
    "        seeds = results[clfstate]\n",
    "        print(f\"{clfstate:12} {len(seeds):8,}\")\n",
    "        total = sum(len(s) for s in results.values())\n",
    "    print(('-'*12) + '-' + ('-'*8))\n",
    "    print(f\"{'Total':12} {total:8,}\") \n",
    "          \n",
    "def get_state(ret):\n",
    "    deck_json = convert_shootme_to_solvitaire_json(ret)\n",
    "    return init_from_solvitaire(deck_json)\n",
    "\n",
    "def map_seeds_to_states(seed_seq):\n",
    "    for seed in seed_seq:\n",
    "        with open(f\"bench/shootme/{seed}\") as f:\n",
    "            ret = f.read()\n",
    "            state = get_state(ret)\n",
    "            yield seed, state\n",
    "\n",
    "print(\"All seeds\")\n",
    "clf_summary(all_solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In features: game state vector of size $233 \\times 104$\n",
    "\n",
    "- Out features: legal moves vector of size $623$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = 233*104\n",
    "OUT = 623\n",
    "\n",
    "Args = namedtuple('Args', 'gamma lr seed render log_interval')\n",
    "args = Args(\n",
    "    gamma=0.5,\n",
    "    lr=5e-2,\n",
    "    seed=1,\n",
    "    render=False,\n",
    "    log_interval=20)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.inlayer = nn.Linear(IN, 233*10)\n",
    "        self.linear1 = nn.Linear(233*10, 1000)\n",
    "        self.outlayer = nn.Linear(1000, OUT)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inlayer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.outlayer(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "def select_action(klonstate):\n",
    "    state_vec = state_to_vec(klonstate)\n",
    "    movefilter = vector_legal_moves(klonstate)\n",
    "    torch_state_vec = torch.from_numpy(state_vec).float().reshape(-1).unsqueeze(0)\n",
    "    torch_filter = torch.from_numpy(movefilter.astype(np.float32)).unsqueeze(0)\n",
    "    probs = policy(torch_state_vec) * torch_filter\n",
    "    if (probs == 0).all():\n",
    "        torch_filter.requires_grad_()\n",
    "        # sample all legal moves with uniform probability\n",
    "        m = Categorical(torch_filter)\n",
    "    else:\n",
    "        # :attr:`probs` will be normalized to sum to 1\n",
    "        m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    policy.saved_log_probs.append(log_prob)\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    \n",
    "def step(curr_state, move_code):\n",
    "    new_state = play_move(curr_state, move_code)\n",
    "    reward = 0  \n",
    "    if state_is_win(new_state) or all_cards_faceup(new_state):\n",
    "        reward = 1\n",
    "    if not state_is_legal(new_state):\n",
    "        print('\\n got illegal state by playing move', move_code)\n",
    "        print('prev state')\n",
    "        print(to_pretty_string(curr_state))\n",
    "        print('\\nnew state')\n",
    "        print(to_pretty_string(new_state))\n",
    "        assert state_is_legal(new_state), 'got illegal state'\n",
    "    return new_state, reward\n",
    "\n",
    "def score_state(klonstate):\n",
    "    fnds = [klonstate.foundation1, klonstate.foundation2, \n",
    "            klonstate.foundation3, klonstate.foundation4]\n",
    "    return sum(map(len, fnds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  ep reward -1.00   steps   7   \n",
      "    1  ep reward -1.00   steps   9   \n",
      "    2  ep reward -1.00   steps  13   \n",
      "    3  ep reward -1.00   steps  26   \n",
      "    4  ep reward -1.00   steps   8   \n",
      "    5  ep reward -1.00   steps   3   \n",
      "    6  ep reward -1.00   steps   7   \n",
      "    7  ep reward -1.00   steps   6   \n",
      "    8  ep reward -1.00   steps   6   \n",
      "    9  ep reward -1.00   steps   4   \n",
      "   10  ep reward -1.00   steps  13   \n",
      "   11  ep reward -1.00   steps   3   \n",
      "   12  ep reward -1.00   steps   5   \n",
      "   13  ep reward -1.00   steps  13   \n",
      "   14  ep reward -1.00   steps   5   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-08f18a46571c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#     print(to_pretty_string(klonstate))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mfinish_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-96e20f1ac207>\u001b[0m in \u001b[0;36mfinish_episode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ada/projects/klonsolve/klonsolve/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ada/projects/klonsolve/klonsolve/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.SGD(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "random.seed(0)\n",
    "# train_seeds = random.sample(all_solutions, k=100)\n",
    "# print('Training game set')\n",
    "# clf_summary(train_seeds)\n",
    "\n",
    "training_games = map_seeds_to_states(all_solutions)\n",
    "\n",
    "for i, env in enumerate(training_games):\n",
    "    seed, klonstate = env\n",
    "#     print(seed)\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    path = []\n",
    "    visited = set()\n",
    "    for t in range(1, 1000):\n",
    "        action_idx = select_action(klonstate)\n",
    "        move_code = all_moves[action_idx]\n",
    "        klonstate, reward = step(klonstate, move_code)\n",
    "        path.append(move_code)\n",
    "        if klonstate in visited:\n",
    "            reward = -1\n",
    "        else:\n",
    "            visited.add(klonstate)\n",
    "        done = reward != 0\n",
    "#         if reward > 0:\n",
    "#             # if it didnt hit a cycle then reward = state score\n",
    "#             reward = score_state(klonstate)\n",
    "#         elif reward < 0: # hit cycle\n",
    "#             # few steps hitting a cycle gets large negative reward\n",
    "#             # many steps hitting a cycle gets a smaller negative reward\n",
    "#             reward = 10/(t * reward)\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "#             print(f'{seed:11} done after {t:4} steps {ep_reward:.2f}')\n",
    "            break\n",
    "    print(f\"{i:5}  ep reward {ep_reward:.2f}   steps {t:3}   \")#, end=' ')\n",
    "#     print('final score', score_state(klonstate))\n",
    "#     print('path', path)\n",
    "#     print('final state')\n",
    "#     print(to_pretty_string(klonstate))\n",
    "    \n",
    "    finish_episode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "klonsolve",
   "language": "python",
   "name": "klonsolve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
