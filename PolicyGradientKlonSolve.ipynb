{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/87d9a1e930b5b813/reinforcement_learning/reinforce.py\n",
    "- https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf [Notebook](https://nbviewer.jupyter.org/urls/gist.githubusercontent.com/ts1829/ebbe2cf946bf36951b724818c52e36b9/raw/4da449bffe9835e201f2fb34f381fbb53568d1ca/Policy%20Gradient%20with%20Cartpole%20and%20PyTorch%20%28Medium%20Version%29.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "\n",
    "from tuplestate import *\n",
    "from get_legal_moves import *\n",
    "from benchmarking import convert_shootme_to_solvitaire_json\n",
    "\n",
    "\n",
    "def listdir(path):\n",
    "    ls = os.listdir(path)\n",
    "    return [os.path.join(path, f) for f in ls]\n",
    "\n",
    "solved = listdir('./fixtures/shootme/solved/')\n",
    "solvedmin = listdir('./fixtures/shootme/solvedmin/')\n",
    "fixtures = solved + solvedmin\n",
    "\n",
    "def filename_to_klonstate(fname):\n",
    "    with open(fname) as f:\n",
    "        solv = convert_shootme_to_solvitaire_json(f.read())\n",
    "        state = init_from_solvitaire(solv)\n",
    "    return state\n",
    "\n",
    "klonstate = filename_to_klonstate(np.random.choice(fixtures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', 'gamma lr seed render log_interval')\n",
    "args = Args(\n",
    "    gamma=0.01,\n",
    "    lr=5e-2,\n",
    "    seed=1,\n",
    "    render=True,\n",
    "    log_interval=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KlonState(stock=('TS', 'AC', '8D', '7D', '6H', 'KH', '8C', '7S', 'KD', 'AS', '7H', '4C', '6C', '5H', '5C', 'KS', '9H', 'JH', '5D', '7C', 'JD', 'KC', '3S', '9S'), tableau1=('4H',), tableau2=('th', 'AH'), tableau3=('4s', '8s', '5S'), tableau4=('qd', '4d', 'qh', '9D'), tableau5=('ad', 'jc', 'td', 'tc', '6D'), tableau6=('qc', '8h', 'js', '9c', '6s', 'QS'), tableau7=('2h', '3d', '2c', '2s', '2d', '3h', '3C'), waste=(), foundation1=(), foundation2=(), foundation3=(), foundation4=())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klonstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([len(klonstate[f]) for f in FNDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KlonEnvironment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.seed(2019)\n",
    "        self.state = None\n",
    "        self.vec_state = None\n",
    "        self.visited = set()\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        \"\"\"\n",
    "        initialize random environment seed\n",
    "        \"\"\"\n",
    "        self.rand = np.random.RandomState(seed)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<KlonEnvironment (visited={len(self.visited)})>\"\n",
    "    \n",
    "    @property\n",
    "    def spec(self):\n",
    "        Spec = namedtuple('spec', 'reward_threshold')\n",
    "        return Spec(\n",
    "            reward_threshold=100\n",
    "        )\n",
    "  \n",
    "    def reset_state(self, state):\n",
    "        self.state = state\n",
    "        self.vec_state = state_to_vec(state)\n",
    "        self.visited = set([ self.state ])\n",
    "        \n",
    "    def update_state(self, new_state):\n",
    "        self.state = new_state\n",
    "        self.vec_state = state_to_vec(new_state)\n",
    "        self.visited.add(new_state)\n",
    "      \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        sets the environment fresh and random\n",
    "        returns:\n",
    "            state\n",
    "        \"\"\"\n",
    "        fixture_file = self.rand.choice(fixtures)\n",
    "        state = filename_to_klonstate(fixture_file)\n",
    "        self.reset_state(state)\n",
    "        return self.vec_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        take the action\n",
    "        returns:\n",
    "            state: new state\n",
    "            reward: reward for taking action\n",
    "            done: is done?\n",
    "            info: ...? (None)\n",
    "        \"\"\"\n",
    "        info = None\n",
    "        done = False\n",
    "        move_code = np_all_moves[action]\n",
    "        new_state = play_move(self.state, move_code)\n",
    "        is_win = state_is_win(new_state)\n",
    "        if new_state in self.visited:\n",
    "            done = True\n",
    "            reward = -100\n",
    "            info = 'state visited'\n",
    "        elif is_win:\n",
    "            done = True\n",
    "            reward = 100\n",
    "            info = 'win!'\n",
    "        else: # non-terminal\n",
    "            # reward is the total num of cards in a foundation\n",
    "            reward = sum([len(new_state[f]) for f in FNDS])\n",
    "        self.update_state(new_state)\n",
    "        return self.vec_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        print(to_dict(self.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "env = KlonEnvironment()\n",
    "env.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In features:\n",
    "- game state (vector of size 233)\n",
    "\n",
    "Out features:\n",
    "- legal moves (vector of size 623)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple feed forward neural network with one hidden layer of 128 neurons and a dropout of 0.6. We'll use Adam as our optimizer and a learning rate of 0.01. Using dropout will significantly improve the performance of our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = 233\n",
    "OUT = 623\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(IN, 500)\n",
    "        self.linear1 = nn.Linear(500, 800)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.affine2 = nn.Linear(800, OUT)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.vec_state\n",
    "# # do pytorch stuff to get state probabilities\n",
    "# torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "# # get probabilities from the last layer of the policy\n",
    "# probs = policy(torchstate)    \n",
    "# # get the vector of legal moves for this particular state\n",
    "# vec = vector_legal_moves(env.state)\n",
    "# # reshape to be a column vector (1x623) instead of 1-D (623,)\n",
    "# nv = vec.reshape(1, -1)\n",
    "# # legal moves are 1, others are 0\n",
    "# # multiply to make illegal moves 0 prob\n",
    "# msk_probs = (probs * torch.Tensor(nv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the vectorized state from the environment\n",
    "# env.reset()\n",
    "# state = env.vec_state\n",
    "# # do pytorch stuff to get state probabilities\n",
    "# torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "# # get probabilities from the last layer of the policy\n",
    "# probs = policy(torchstate)    \n",
    "# # get the vector of legal moves for this particular state\n",
    "# vec = vector_legal_moves(env.state)\n",
    "# # reshape to be a column vector (1x623) instead of 1-D (623,)\n",
    "# nv = vec.reshape(1, -1)\n",
    "\n",
    "# if torch.isnan(probs).all(): # this happens! cant sample from Categorical nans\n",
    "#     # sample random legal move\n",
    "#     print(\"all nans choose random\")\n",
    "#     m = Categorical(torch.Tensor(nv))\n",
    "# else:\n",
    "#     # legal moves are 1, others are 0\n",
    "#     # multiply to make illegal moves 0 prob\n",
    "#     msk_probs = (probs * torch.Tensor(nv))\n",
    "#     # still 623 probabilities but most are 0\n",
    "#     # did not seem to require normalizing probabilities to 1\n",
    "#     print('got a real prob')\n",
    "#     m = Categorical(msk_probs)\n",
    "\n",
    "# action = m.sample()\n",
    "# 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk_probs = (probs * torch.Tensor(nv))\n",
    "# # still 623 probabilities but most are 0\n",
    "# # did not seem to require normalizing probabilities to 1\n",
    "# m = Categorical(msk_probs)\n",
    "# action = m.sample()\n",
    "# m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk_probs.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnv = torch.Tensor(nv)\n",
    "# tnv.requires_grad_()\n",
    "# m = Categorical(tnv)\n",
    "# action = m.sample()\n",
    "# m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (encountering probability entry < 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f899b2d4d051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-f899b2d4d051>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't infinite loop while learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#             if info:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f899b2d4d051>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#         print('got a real prob')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsk_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# DEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/glifchits/miniconda3/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprobs_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0msample_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid multinomial distribution (encountering probability entry < 0)"
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=args.lr)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(env):\n",
    "    # get the vectorized state from the environment\n",
    "    state = env.vec_state\n",
    "    # do pytorch stuff to get state probabilities\n",
    "    torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    # get probabilities from the last layer of the policy\n",
    "    probs = policy(torchstate)    \n",
    "    # get the vector of legal moves for this particular state\n",
    "    vec = vector_legal_moves(env.state)\n",
    "    # reshape to be a column vector (1x623) instead of 1-D (623,)\n",
    "    tnv = torch.Tensor(vec.reshape(1, -1))\n",
    "    \n",
    "    if torch.isnan(probs).all(): # this happens! cant sample from Categorical nans\n",
    "        # sample random legal move\n",
    "#         print(\"all nans choose random\")\n",
    "        tnv.requires_grad_() # so m.log_prob(action) is diffable\n",
    "        m = Categorical(tnv)\n",
    "        action = m.sample()\n",
    "    else:\n",
    "        # legal moves are 1, others are 0\n",
    "        # multiply to make illegal moves 0 prob\n",
    "        msk_probs = (probs * tnv)\n",
    "        # still 623 probabilities but most are 0\n",
    "        # did not seem to require normalizing probabilities to 1\n",
    "#         print('got a real prob')\n",
    "        m = Categorical(msk_probs)\n",
    "        action = m.sample()\n",
    "    \n",
    "    # DEBUG\n",
    "#     print('selecting', np_all_moves[action.tolist()[0]])\n",
    "    # end debug\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in count(1):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(env)\n",
    "            state, reward, done, info = env.step(action)\n",
    "#             if info:\n",
    "#                 print(info)\n",
    "#             if args.render:\n",
    "#                 env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "#         for n,p in policy.named_parameters():\n",
    "#             print(n, p.mean(), p.var())\n",
    "#         print()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    gamma=0.01,\n",
    "    lr=3e-2,\n",
    "    seed=1,\n",
    "    render=True,\n",
    "    log_interval=20\n",
    ")\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=args.lr)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stock': ['QC', '5C', 'AS', 'JS', 'KS', '9D', '5D', '3S', '6C', '8H', '6D', '9C', 'TD', 'KH', '7H', 'JH', 'QS', '8D', '3C', 'QD', '4S', '5S', '7S', 'AH'], 'waste': [], 'tableau': [['3H'], ['9h', 'JC'], ['7d', '4c', '4D'], ['2d', 'ad', 'tc', '2C'], ['kc', '6h', '2h', 'ts', 'KD'], ['ac', '5h', '3d', '9s', '7c', '2S'], ['4h', '8s', 'jd', 'qh', 'th', '8c', '6S']], 'foundation': [[], [], [], []]}\n",
      "as usual\n",
      "action tensor([616])\n",
      "as usual\n",
      "action tensor([616])\n",
      "as usual\n",
      "action tensor([616])\n",
      "returns [-0.01, -1.0, -100.0]\n",
      "returns scaled tensor([ 0.5859,  0.5687, -1.1547])\n",
      "policy loss [tensor([6.9850e-08], grad_fn=<MulBackward0>), tensor([6.7796e-08], grad_fn=<MulBackward0>), tensor([-1.3765e-07], grad_fn=<MulBackward0>)]\n",
      "policy loss sum tensor(0., grad_fn=<SumBackward0>)\n",
      "affine1.weight tensor(0.0012, grad_fn=<MeanBackward0>) tensor(0.0077, grad_fn=<VarBackward0>)\n",
      "affine1.bias tensor(0.0036, grad_fn=<MeanBackward0>) tensor(0.0224, grad_fn=<VarBackward0>)\n",
      "linear1.weight tensor(9.4251e-05, grad_fn=<MeanBackward0>) tensor(0.0109, grad_fn=<VarBackward0>)\n",
      "linear1.bias tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(0.0183, grad_fn=<VarBackward0>)\n",
      "affine2.weight tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(0.0008, grad_fn=<VarBackward0>)\n",
      "affine2.bias tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(0.0009, grad_fn=<VarBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state, ep_reward = env.reset(), 0\n",
    "print(to_dict(vec_to_state(state)))\n",
    "\n",
    "for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "    #     action = select_action(env)\n",
    "    state = env.vec_state\n",
    "    torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(torchstate)    \n",
    "    vec = vector_legal_moves(env.state)\n",
    "    tnv = torch.Tensor(vec.reshape(1, -1))\n",
    "    \n",
    "    if torch.isnan(probs).all():\n",
    "        tnv.requires_grad_() # so m.log_prob(action) is diffable\n",
    "        print('all isnan')\n",
    "        m = Categorical(tnv)\n",
    "        action = m.sample()\n",
    "    else:\n",
    "        msk_probs = (probs * tnv)\n",
    "        print('as usual')\n",
    "        m = Categorical(msk_probs)\n",
    "        action = m.sample()\n",
    "    print('action', action)\n",
    "\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    action = action.item()\n",
    "\n",
    "    state, reward, done, info = env.step(action)\n",
    "    policy.rewards.append(reward)\n",
    "    ep_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "R = 0\n",
    "policy_loss = []\n",
    "returns = []\n",
    "for r in policy.rewards[::-1]:\n",
    "    R = r + args.gamma * R\n",
    "    returns.insert(0, R)\n",
    "\n",
    "print('returns', returns)\n",
    "returns = torch.tensor(returns)\n",
    "returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "print('returns scaled', returns)\n",
    "for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "    policy_loss.append(-log_prob * R)\n",
    "print('policy loss', policy_loss)\n",
    "optimizer.zero_grad()\n",
    "policy_loss = torch.cat(policy_loss).sum()\n",
    "print('policy loss sum', policy_loss)\n",
    "policy_loss.backward()\n",
    "optimizer.step()\n",
    "del policy.rewards[:]\n",
    "del policy.saved_log_probs[:]\n",
    "\n",
    "for n,p in policy.named_parameters():\n",
    "    print(n, p.mean(), p.var())\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
