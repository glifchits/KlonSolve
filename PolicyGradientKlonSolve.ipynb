{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/87d9a1e930b5b813/reinforcement_learning/reinforce.py\n",
    "- https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf [Notebook](https://nbviewer.jupyter.org/urls/gist.githubusercontent.com/ts1829/ebbe2cf946bf36951b724818c52e36b9/raw/4da449bffe9835e201f2fb34f381fbb53568d1ca/Policy%20Gradient%20with%20Cartpole%20and%20PyTorch%20%28Medium%20Version%29.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tuplestate import *\n",
    "from gamestate import *\n",
    "from benchmarking import random_solved_endgame\n",
    "from vectorize import *\n",
    "random.seed(0)\n",
    "klonstate = random_solved_endgame(19)\n",
    "# print(to_pretty_string(klonstate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', 'gamma lr seed render log_interval')\n",
    "args = Args(\n",
    "    gamma=0.01,\n",
    "    lr=5e-2,\n",
    "    seed=1,\n",
    "    render=False,\n",
    "    log_interval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In features: game state vector of size $233 \\times 104$\n",
    "\n",
    "- Out features: legal moves vector of size $623$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = 233*104\n",
    "OUT = 623\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(IN, 800)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(800, OUT)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(klonstate):\n",
    "    state_vec = state_to_vec(klonstate)\n",
    "    movefilter = vector_legal_moves(klonstate)\n",
    "    torch_state_vec = torch.from_numpy(state_vec).float().reshape(-1).unsqueeze(0)\n",
    "    torch_filter = torch.from_numpy(movefilter.astype(np.float32)).unsqueeze(0)\n",
    "    probs = policy(torch_state_vec) * torch_filter\n",
    "    if (probs == 0).all():\n",
    "        torch_filter.requires_grad_()\n",
    "        # sample all legal moves with uniform probability\n",
    "        m = Categorical(torch_filter)\n",
    "    else:\n",
    "        # :attr:`probs` will be normalized to sum to 1\n",
    "        m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    policy.saved_log_probs.append(log_prob)\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    tcpl = torch.cat(policy_loss)\n",
    "    policy_loss = -tcpl.sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    \n",
    "def step(curr_state, action_idx):\n",
    "    move_code = all_moves[action_idx]\n",
    "    new_state = play_move(curr_state, move_code)\n",
    "    reward = 0  \n",
    "    if state_is_win(new_state) or all_cards_faceup(new_state):\n",
    "        reward = 1\n",
    "    if not state_is_legal(new_state):\n",
    "        print('\\n got illegal state by playing move', move_code)\n",
    "        print('prev state')\n",
    "        print(to_pretty_string(curr_state))\n",
    "        print('\\nnew state')\n",
    "        print(to_pretty_string(new_state))\n",
    "        assert state_is_legal(new_state), 'got illegal state'\n",
    "    return new_state, reward\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.SGD(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done after    3 steps 1.00\n",
      "done after   32 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   21 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    5 steps 1.00\n",
      "done after    6 steps 1.00\n",
      "done after   12 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   14 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    3 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   29 steps 1.00\n",
      "done after   13 steps 1.00\n",
      "done after   42 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    9 steps 1.00\n",
      "done after   45 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    2 steps 1.00\n",
      "done after   19 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   21 steps 1.00\n",
      "done after   10 steps 1.00\n",
      "done after    3 steps 1.00\n",
      "done after   99 steps 1.00\n",
      "done after   30 steps 1.00\n",
      "done after   11 steps 1.00\n",
      "done after    9 steps 1.00\n",
      "done after   14 steps 1.00\n",
      "done after    8 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   17 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   12 steps 1.00\n",
      "done after   21 steps 1.00\n",
      "done after   55 steps 1.00\n",
      "done after   12 steps 1.00\n",
      "done after   47 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   13 steps 1.00\n",
      "done after    7 steps 1.00\n",
      "done after   24 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    3 steps 1.00\n",
      "done after   12 steps 1.00\n",
      "done after   10 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   15 steps 1.00\n",
      "done after   12 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   23 steps 1.00\n",
      "done after   10 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   32 steps 1.00\n",
      "done after    7 steps 1.00\n",
      "done after   15 steps 1.00\n",
      "done after    9 steps 1.00\n",
      "done after   40 steps 1.00\n",
      "done after    8 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    8 steps 1.00\n",
      "done after   11 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after   41 steps 1.00\n",
      "done after   11 steps 1.00\n",
      "done after   10 steps 1.00\n",
      "done after   15 steps 1.00\n",
      "done after   14 steps 1.00\n",
      "done after   14 steps 1.00\n",
      "done after    7 steps 1.00\n",
      "done after    4 steps 1.00\n",
      "done after   10 steps 1.00\n",
      "done after   31 steps 1.00\n",
      "done after    3 steps 1.00\n",
      "done after    1 steps 1.00\n",
      "done after    4 steps 1.00\n",
      "done after    9 steps 1.00\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1, 100):\n",
    "    klonstate = random_solved_endgame(20)\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    for t in range(1, 100):\n",
    "        action = select_action(klonstate)\n",
    "        klonstate, reward = step(klonstate, action)\n",
    "        done = reward > 0\n",
    "        policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            print(f'{i} done after {t:4} steps {ep_reward:.2f}')\n",
    "            break\n",
    "    finish_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: JH\n",
      "Waste: QD TS QC KD 9H JS\n",
      "Fnd C: AC 2C 3C 4C 5C 6C 7C 8C 9C\n",
      "Fnd D: AD 2D 3D 4D 5D 6D 7D 8D 9D TD\n",
      "Fnd S: AS 2S 3S 4S 5S 6S 7S 8S 9S\n",
      "Fnd H: AH 2H 3H 4H 5H 6H 7H 8H\n",
      "Tab 1: TH\n",
      "Tab 2: KH\n",
      "Tab 3: QS JD TC\n",
      "Tab 4: KC\n",
      "Tab 5: \n",
      "Tab 6: KS\n",
      "Tab 7: qh JC\n",
      "\n",
      "\n",
      "Not solved after 500 iterations\n",
      "Stock: JS 9H KD\n",
      "Waste: QD TS QC\n",
      "Fnd C: AC 2C 3C 4C 5C 6C 7C 8C 9C\n",
      "Fnd D: AD 2D 3D 4D 5D 6D 7D 8D 9D TD JD\n",
      "Fnd S: AS 2S 3S 4S 5S 6S 7S 8S 9S\n",
      "Fnd H: AH 2H 3H 4H 5H 6H 7H 8H\n",
      "Tab 1: \n",
      "Tab 2: KH QS JH TC\n",
      "Tab 3: \n",
      "Tab 4: KC\n",
      "Tab 5: \n",
      "Tab 6: KS\n",
      "Tab 7: qh JC TH\n"
     ]
    }
   ],
   "source": [
    "klonstate = random_solved_endgame(20)\n",
    "print(to_pretty_string(klonstate))\n",
    "print()\n",
    "\n",
    "done = False\n",
    "visited = set()\n",
    "for i in range(500):\n",
    "    visited.add(klonstate)\n",
    "    action = select_action(klonstate)\n",
    "#     print(f'iteration {i}  action {all_moves[action]}')\n",
    "    klonstate, reward = step(klonstate, action)\n",
    "    done = reward > 0\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "if done:\n",
    "    print(\"Solved!\")\n",
    "else:\n",
    "    print(f'Not solved after {i+1} iterations')\n",
    "    \n",
    "print(to_pretty_string(klonstate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
