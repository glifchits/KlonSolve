{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/87d9a1e930b5b813/reinforcement_learning/reinforce.py\n",
    "- https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf [Notebook](https://nbviewer.jupyter.org/urls/gist.githubusercontent.com/ts1829/ebbe2cf946bf36951b724818c52e36b9/raw/4da449bffe9835e201f2fb34f381fbb53568d1ca/Policy%20Gradient%20with%20Cartpole%20and%20PyTorch%20%28Medium%20Version%29.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "\n",
    "from tuplestate import *\n",
    "from get_legal_moves import *\n",
    "from benchmarking import convert_shootme_to_solvitaire_json\n",
    "\n",
    "\n",
    "def listdir(path):\n",
    "    ls = os.listdir(path)\n",
    "    return [os.path.join(path, f) for f in ls]\n",
    "\n",
    "solved = listdir('./fixtures/shootme/solved/')\n",
    "solvedmin = listdir('./fixtures/shootme/solvedmin/')\n",
    "fixtures = solved + solvedmin\n",
    "\n",
    "def filename_to_klonstate(fname):\n",
    "    with open(fname) as f:\n",
    "        solv = convert_shootme_to_solvitaire_json(f.read())\n",
    "        state = init_from_solvitaire(solv)\n",
    "    return state\n",
    "\n",
    "klonstate = filename_to_klonstate(np.random.choice(fixtures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', 'gamma lr seed render log_interval')\n",
    "args = Args(\n",
    "    gamma=0.01,\n",
    "    lr=1e-3,\n",
    "    seed=1,\n",
    "    render=True,\n",
    "    log_interval=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KlonEnvironment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.seed(2019)\n",
    "        self.state = None\n",
    "        self.vec_state = None\n",
    "        self.visited = set()\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        \"\"\"\n",
    "        initialize random environment seed\n",
    "        \"\"\"\n",
    "        self.rand = np.random.RandomState(seed)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<KlonEnvironment (visited={len(self.visited)})>\"\n",
    "    \n",
    "    @property\n",
    "    def spec(self):\n",
    "        Spec = namedtuple('spec', 'reward_threshold')\n",
    "        return Spec(\n",
    "            reward_threshold=1\n",
    "        )\n",
    "  \n",
    "    def reset_state(self, state):\n",
    "        self.state = state\n",
    "        self.vec_state = state_to_vec(state)\n",
    "        self.visited = set([ self.state ])\n",
    "        \n",
    "    def update_state(self, new_state):\n",
    "        self.state = new_state\n",
    "        self.vec_state = state_to_vec(new_state)\n",
    "        self.visited.add(new_state)\n",
    "      \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        sets the environment fresh and random\n",
    "        returns:\n",
    "            state\n",
    "        \"\"\"\n",
    "        fixture_file = self.rand.choice(fixtures)\n",
    "        state = filename_to_klonstate(fixture_file)\n",
    "        self.reset_state(state)\n",
    "        return self.vec_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        take the action\n",
    "        returns:\n",
    "            state: new state\n",
    "            reward: reward for taking action\n",
    "            done: is done?\n",
    "            info: ...? (None)\n",
    "        \"\"\"\n",
    "        info = None\n",
    "        done = False\n",
    "        reward = 0\n",
    "        move_code = np_all_moves[action]\n",
    "        new_state = play_move(self.state, move_code)\n",
    "        is_win = state_is_win(new_state)\n",
    "        if new_state in self.visited:\n",
    "            done = True\n",
    "            reward = -1\n",
    "            info = 'state visited'\n",
    "        elif is_win:\n",
    "            done = True\n",
    "            reward = 1\n",
    "            info = 'win!'\n",
    "        self.update_state(new_state)\n",
    "        return self.vec_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        print(to_dict(self.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "env = KlonEnvironment()\n",
    "env.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In features:\n",
    "- game state (vector of size 233)\n",
    "\n",
    "Out features:\n",
    "- legal moves (vector of size 623)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple feed forward neural network with one hidden layer of 128 neurons and a dropout of 0.6. We'll use Adam as our optimizer and a learning rate of 0.01. Using dropout will significantly improve the performance of our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = 233\n",
    "OUT = 623\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(IN, 500)\n",
    "        self.linear1 = nn.Linear(500, 800)\n",
    "        self.linear2 = nn.Linear(800, 800)\n",
    "        self.linear3 = nn.Linear(800, 800)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.affine2 = nn.Linear(800, OUT)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "    \n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=args.lr)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.vec_state\n",
    "# # do pytorch stuff to get state probabilities\n",
    "# torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "# # get probabilities from the last layer of the policy\n",
    "# probs = policy(torchstate)    \n",
    "# # get the vector of legal moves for this particular state\n",
    "# vec = vector_legal_moves(env.state)\n",
    "# # reshape to be a column vector (1x623) instead of 1-D (623,)\n",
    "# nv = vec.reshape(1, -1)\n",
    "# # legal moves are 1, others are 0\n",
    "# # multiply to make illegal moves 0 prob\n",
    "# msk_probs = (probs * torch.Tensor(nv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the vectorized state from the environment\n",
    "# env.reset()\n",
    "# state = env.vec_state\n",
    "# # do pytorch stuff to get state probabilities\n",
    "# torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "# # get probabilities from the last layer of the policy\n",
    "# probs = policy(torchstate)    \n",
    "# # get the vector of legal moves for this particular state\n",
    "# vec = vector_legal_moves(env.state)\n",
    "# # reshape to be a column vector (1x623) instead of 1-D (623,)\n",
    "# nv = vec.reshape(1, -1)\n",
    "\n",
    "# if torch.isnan(probs).all(): # this happens! cant sample from Categorical nans\n",
    "#     # sample random legal move\n",
    "#     print(\"all nans choose random\")\n",
    "#     m = Categorical(torch.Tensor(nv))\n",
    "# else:\n",
    "#     # legal moves are 1, others are 0\n",
    "#     # multiply to make illegal moves 0 prob\n",
    "#     msk_probs = (probs * torch.Tensor(nv))\n",
    "#     # still 623 probabilities but most are 0\n",
    "#     # did not seem to require normalizing probabilities to 1\n",
    "#     print('got a real prob')\n",
    "#     m = Categorical(msk_probs)\n",
    "\n",
    "# action = m.sample()\n",
    "# 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk_probs = (probs * torch.Tensor(nv))\n",
    "# # still 623 probabilities but most are 0\n",
    "# # did not seem to require normalizing probabilities to 1\n",
    "# m = Categorical(msk_probs)\n",
    "# action = m.sample()\n",
    "# m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk_probs.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnv = torch.Tensor(nv)\n",
    "# tnv.requires_grad_()\n",
    "# m = Categorical(tnv)\n",
    "# action = m.sample()\n",
    "# m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def select_action(env):\n",
    "    # get the vectorized state from the environment\n",
    "    state = env.vec_state\n",
    "    # do pytorch stuff to get state probabilities\n",
    "    torchstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    # get probabilities from the last layer of the policy\n",
    "    probs = policy(torchstate)    \n",
    "    # get the vector of legal moves for this particular state\n",
    "    vec = vector_legal_moves(env.state)\n",
    "    # reshape to be a column vector (1x623) instead of 1-D (623,)\n",
    "    tnv = torch.Tensor(vec.reshape(1, -1))\n",
    "    \n",
    "    if torch.isnan(probs).all(): # this happens! cant sample from Categorical nans\n",
    "        # sample random legal move\n",
    "#         print(\"all nans choose random\")\n",
    "        tnv.requires_grad_() # so m.log_prob(action) is diffable\n",
    "        m = Categorical(tnv)\n",
    "        action = m.sample()\n",
    "    else:\n",
    "        # legal moves are 1, others are 0\n",
    "        # multiply to make illegal moves 0 prob\n",
    "        msk_probs = (probs * tnv)\n",
    "        # still 623 probabilities but most are 0\n",
    "        # did not seem to require normalizing probabilities to 1\n",
    "#         print('got a real prob')\n",
    "        m = Categorical(msk_probs)\n",
    "        action = m.sample()\n",
    "    \n",
    "    # DEBUG\n",
    "#     print('selecting', np_all_moves[action.tolist()[0]])\n",
    "    # end debug\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in count(1):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(env)\n",
    "            state, reward, done, info = env.step(action)\n",
    "#             if info:\n",
    "#                 print(info)\n",
    "#             if args.render:\n",
    "#                 env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %pdb\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 0\n",
    "state, ep_reward = env.reset(), 0\n",
    "for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "    action = select_action(env)\n",
    "    state, reward, done, info = env.step(action)\n",
    "#             if info:\n",
    "#                 print(info)\n",
    "#             if args.render:\n",
    "#                 env.render()\n",
    "    policy.rewards.append(reward)\n",
    "    ep_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = Args(\n",
    "#     gamma=0.01,\n",
    "#     lr=3e-2,\n",
    "#     seed=1,\n",
    "#     render=True,\n",
    "#     log_interval=20\n",
    "# )\n",
    "# R = 0\n",
    "# policy_loss = []\n",
    "# returns = []\n",
    "# for r in policy.rewards[::-1]:\n",
    "#     R = r + args.gamma * R\n",
    "#     returns.insert(0, R)\n",
    "# returns = torch.tensor(returns)\n",
    "# returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "# for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "#     policy_loss.append(-log_prob * R)\n",
    "# # optimizer.zero_grad()\n",
    "# # policy_loss = torch.cat(policy_loss).sum()\n",
    "# # policy_loss.backward()\n",
    "# # optimizer.step()\n",
    "# # del policy.rewards[:]\n",
    "# # del policy.saved_log_probs[:]\n",
    "# returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
